[
  {
    "modelName": "qwen2.5-coder:latest",
    "size": "4.7 GB",
    "parameter_size": "7B",
    "quantization_level": "Q4_K_M"
  },
  {
    "modelName": "llama3.1:8b-instruct-q8_0",
    "size": "8.5 GB",
    "parameter_size": "8B",
    "quantization_level": "Q8_0"
  },
  {
    "modelName": "minicpm-v:latest",
    "size": "5.5 GB",
    "parameter_size": "8B",
    "quantization_level": "Q4_0"
  },
  {
    "modelName": "taozhiyuai/llama-3-8b-lexi-uncensored:q5_k_m",
    "size": "5.7 GB",
    "parameter_size": "8B",
    "quantization_level": "Q5_K_M"
  },
  {
    "modelName": "gurubot/llama3-guru-uncensored:latest",
    "size": "4.9 GB",
    "parameter_size": "8B",
    "quantization_level": "Q4_K_M"
  },
  {
    "modelName": "mannix/llama3-uncensored:latest",
    "size": "4.7 GB",
    "parameter_size": "8B",
    "quantization_level": "Q4_0"
  },
  {
    "modelName": "wizardlm-uncensored:latest",
    "size": "7.4 GB",
    "parameter_size": "13B",
    "quantization_level": "Q4_0"
  },
  {
    "modelName": "llama3:latest",
    "size": "4.7 GB",
    "parameter_size": "8B",
    "quantization_level": "Q4_0"
  },
  {
    "modelName": "mixtral:8x7b",
    "size": "26 GB",
    "parameter_size": "46.7B",
    "quantization_level": "Q4_0"
  },
  {
    "modelName": "wizardlm2:latest",
    "size": "4.1 GB",
    "parameter_size": "7B",
    "quantization_level": "Q4_0"
  },
  {
    "modelName": "open-orca-platypus2:latest",
    "size": "7.4 GB",
    "parameter_size": "13B",
    "quantization_level": "Q4_0"
  },
  {
    "modelName": "orca2:latest",
    "size": "3.8 GB",
    "parameter_size": "7B",
    "quantization_level": "Q4_0"
  },
  {
    "modelName": "meditron:latest",
    "size": "3.8 GB",
    "parameter_size": "7B",
    "quantization_level": "Q4_0"
  },
  {
    "modelName": "medllama2:latest",
    "size": "3.8 GB",
    "parameter_size": "7B",
    "quantization_level": "Q4_0"
  },
  {
    "modelName": "medllama2:7b-q8_0",
    "size": "7.2 GB",
    "parameter_size": "7B",
    "quantization_level": "Q8_0"
  },
  {
    "modelName": "mistral-openorca:latest",
    "size": "4.1 GB",
    "parameter_size": "7B",
    "quantization_level": "Q4_0"
  },
  {
    "modelName": "llama2:latest",
    "size": "3.8 GB",
    "parameter_size": "7B",
    "quantization_level": "Q4_0"
  },
  {
    "modelName": "llama2-uncensored:latest",
    "size": "3.8 GB",
    "parameter_size": "7B",
    "quantization_level": "Q4_0"
  },
  {
    "modelName": "stable-beluga:latest",
    "size": "3.8 GB",
    "parameter_size": "7B",
    "quantization_level": "Q4_0"
  },
  {
    "modelName": "mistral:latest",
    "size": "4.1 GB",
    "parameter_size": "7B",
    "quantization_level": "Q4_0"
  },
  {
    "modelName": "orca-mini:latest",
    "size": "2.0 GB",
    "parameter_size": "3B",
    "quantization_level": "Q4_0"
  },
  {
    "modelName": "hermes3:8b-llama3.1-q8_0",
    "size": "8.5 GB",
    "parameter_size": "8B",
    "quantization_level": "Q8_0"
  },
  {
    "modelName": "deepseek-r1:8b-llama-distill-q8_0",
    "size": "8.5 GB",
    "parameter_size": "8B",
    "quantization_level": "Q8_0"
  },
  {
    "modelName": "deepseek-r1:14b-qwen-distill-q8_0",
    "size": "16.0 GB",
    "parameter_size": "14.7B",
    "quantization_level": "Q8_0"
  }
]